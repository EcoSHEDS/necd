# SHEDS - Daymet Dataset

Jeffrey D Walker, PhD  
[Walker Environmental Research LLC](https://walkerenvres.com)

## Initialize Database

Set wd to `db/`

```bash
cd db
```

Create new database

```bash
createdb daymet
```

Install schema

```bash
psql -d daymet -f schema.sql
```

Install functions

```bash
psql -d daymet -f functions.sql
```

## Daymet Update Workflow

### 1. Reproject catchments

NOTE: only needs to be done once to generate `Catchments0X_Daymet.shp`

Reproject catchments to Lambert Conformal Conic ([Daymet projection](https://daymet.ornl.gov/datasupport.html))

```bash
cd scripts
./project-catchments.shp /path/to/shp/ # /path/to/shp/Catchments0X.shp -> /path/to/shp/Catchments0X_Daymet.shp
proj4: "+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs"
```

### 2. Download Mosaic Layers

Download mosaic layers (NetCDF) from the [https://daymet.ornl.gov/](Daymet) [https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/1328/catalog.html](THREDDS Data Server). Use "Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 3" dataset.

```bash
cd scripts
# for a single year
./download-daymet-year.sh 2018 /path/to/daymet/nc4
# for multiple years
for YEAR in {1980..2017}; do ./download-daymet-year.sh $YEAR ../data/nc4; done
```

### 3. Process Catchment Averages

Run R script `r/process-daymet.R` to compute average values by catchment and store results in SQLite database

```bash
cd r
Rscript process-daymet.R --dir_nc4=/path/to/daymet/nc4 --dir_db=/path/to/daymet/db/2018 --dir_shp=/path/to/gis/NHDHRDV2/catchments --start_year 2018 --end_year 2018
# or
# open r/process-daymet.R in RStudio and run interactively
```

### 4. Import Catchment Averages

Set up `daymet_import` table using `db/schema.sql` (only once, copy and paste).

Import single sqlite database (generated by r/process-daymet.sh)

```bash
./import_sqlite.sh daymet /path/to/SQLITE_DB
```

Import multiple sqlite databases

```bash
nano sqlite_files.txt # list of sqlite db databases
./import_batch.sh daymet sqlite_files.txt
```

## Reference

```
# osensei
Catchment Shapefiles: /data/jeff/gis/NHDHRDV2/catchments
Daymet NC4 Files: /data/jeff/daymet/nc4
SQLite Files: /data/jeff/daymet/db

# felek
SQLite Files: /data/daymet/<YYYY-MM-DD>/

# copy sqlite files from osensei to felek
rsync -av /data/jeff/daymet/db/2018 jeff@ecosheds.org:/data/daymet/20191120/
```

## Kyle's Original Notes

[https://github.com/Conte-Ecology/projectRoadmap/blob/master/project_roadmap_page.md]()

The raw Daymet gridded observed climate data is assigned to the NHDHRDV2 catchments and uploaded to the sheds Postgres database. This process is completed using the custom zonalDaymet R package along with a series of shell scripts. The zonalDaymet package has functions to download and process the raw climate NetCDFs, assign climate records to catchment featureids, and save to a SQLite database. The SQLite databases are then uploaded to for upload to osensei.

Links:

- [Daymet Workflow Repository](https://github.com/Conte-Ecology/shedsGisData/tree/master/daymet)
- [Project Page](http://conte-ecology.github.io/shedsGisData/)
- [zonalDaymet Package Repository](https://github.com/Conte-Ecology/zonalDaymet)
- Data Directory: \IGSAGBEBWS-MJO7\projects\dataIn\environmental\climate\daymet
- Local zonalDaymet Directory: C:\KPONEIL\GitHub\packages\zonalDaymet
